# Inference

Inference is a gRPC-based server tailored for running ONNX models on edge devices, such as Android, iOS, or other
microcontroller units (MCUs).

- Onnx Runtime

Platform:

- Android, iOS
- Linux, Windows, Mac,
- Raspberry Pi, MCU

## Todos

- [ ] GRPC server with [tonic](https://github.com/hyperium/tonic)
- [ ] Onnx Runtime
- [ ] Tokenizer
- [ ] Flexible Configuration: Easily configurable via command-line parameters, including listening port, batch size,
  thread count, and others.

## License

This project is licensed under the MIT License, See [LICENSE](LICENSE) for the full license text.
